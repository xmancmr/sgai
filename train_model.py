# backend/train_model_final.py
import os
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Dropout, BatchNormalization, LayerNormalization
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (
    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
)
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.initializers import HeUniform
from tensorflow.keras.losses import Huber  # Correction importante
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import (
    StandardScaler, LabelEncoder, PowerTransformer
)
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import mutual_info_regression
import joblib
import glob
import json
import re
import warnings
import shutil
import unicodedata
from collections import defaultdict

# D√©sactiver les warnings
warnings.filterwarnings('ignore')

# 1. Configuration
class Config:
    SEED = 42
    TEST_SIZE = 0.15
    VAL_SIZE = 0.15
    BATCH_SIZE = 64
    MAX_EPOCHS = 300
    PATIENCE = 20
    LEARNING_RATE = 0.001
    MIN_LR = 1e-6
    DROPOUT_RATE = 0.3
    L1_REG = 1e-5
    L2_REG = 1e-4
    EMBEDDING_SIZE = 8
    MODEL_SAVE_PATH = 'models/production_model_final.h5'
    TARGET_KEYWORDS = ['production', 'prod', 'output', 'yield', 'quantite', 'volume', 'rendement']
    NUMERIC_FALLBACKS = ['production', 'prod', 'yield']

config = Config()
np.random.seed(config.SEED)
tf.random.set_seed(config.SEED)

# Fonction pour normaliser les cha√Ænes de caract√®res
def normalize_string(s):
    """Normalise une cha√Æne pour la comparaison"""
    if not isinstance(s, str):
        s = str(s)
    s = unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore').decode('utf-8')
    s = s.lower().strip()
    s = re.sub(r'[^a-z0-9]', '', s)
    return s

# 2. Inspection des fichiers CSV
def inspect_csv_files():
    """Inspecte tous les fichiers CSV pour identifier les colonnes disponibles"""
    data_dir = "data"
    all_files = glob.glob(os.path.join(data_dir, "*.csv"))
    
    if not all_files:
        raise FileNotFoundError(f"Aucun fichier CSV trouv√© dans {data_dir}")
    
    column_report = defaultdict(list)
    
    print("\n" + "="*80)
    print("INSPECTION DES FICHIERS CSV")
    print("="*80)
    
    for file in all_files:
        try:
            # Lire juste l'en-t√™te
            with open(file, 'r', encoding='utf-8', errors='replace') as f:
                header = f.readline().strip()
            
            columns = header.split(',')
            filename = os.path.basename(file)
            
            print(f"\nFichier: {filename}")
            print(f"Colonnes: {columns}")
            
            for col in columns:
                column_report[col].append(filename)
                
        except Exception as e:
            print(f"Erreur avec {file}: {str(e)}")
    
    return column_report

# 3. Chargement des fichiers CSV
def load_all_data():
    """Charge tous les fichiers CSV disponibles sans cible sp√©cifique"""
    data_dir = "data"
    all_files = glob.glob(os.path.join(data_dir, "*.csv"))
    
    if not all_files:
        raise FileNotFoundError(f"Aucun fichier CSV trouv√© dans {data_dir}")
    
    dfs = []
    for file in all_files:
        try:
            # Essayer plusieurs encodages
            for encoding in ['utf-8', 'latin1', 'ISO-8859-1']:
                try:
                    df = pd.read_csv(file, encoding=encoding, on_bad_lines='skip')
                    print(f"Charg√©: {file} (encodage: {encoding})")
                    break
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    print(f"Erreur avec {file} ({encoding}): {str(e)}")
            
            # Ajouter le nom du fichier comme colonne
            df['source_file'] = os.path.basename(file)
            dfs.append(df)
        except Exception as e:
            print(f"Erreur avec {file}: {str(e)}")
    
    # Combiner tous les DataFrames
    if not dfs:
        raise ValueError("Aucun DataFrame valide n'a pu √™tre charg√©")
    
    combined_df = pd.concat(dfs, ignore_index=True, sort=False)
    print(f"\nTotal des donn√©es combin√©es: {len(combined_df)} lignes, {len(combined_df.columns)} colonnes")
    
    return combined_df

# 4. S√©lection de la colonne cible
def select_target_column(df):
    """S√©lectionne automatiquement une colonne cible num√©rique"""
    # Identifier les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    
    # 1. Chercher les colonnes avec mots-cl√©s pertinents
    for col in df.columns:
        normalized_col = normalize_string(col)
        for keyword in config.TARGET_KEYWORDS:
            if keyword in normalized_col and col in numeric_cols:
                print(f"S√©lection cible par mot-cl√©: '{col}'")
                return col
    
    # 2. Chercher les colonnes avec des noms de produits num√©riques
    production_like_cols = [col for col in numeric_cols 
                            if any(kw in normalize_string(col) 
                            for kw in ['production', 'prod', 'yield'])]
    
    if production_like_cols:
        print(f"S√©lection cible par similarit√©: '{production_like_cols[0]}'")
        return production_like_cols[0]
    
    # 3. Fallback: Premi√®re colonne num√©rique
    if numeric_cols:
        print(f"S√©lection cible par d√©faut: '{numeric_cols[0]}'")
        return numeric_cols[0]
    
    # 4. Dernier recours: Convertir une colonne
    for col in df.columns:
        try:
            # Tenter la conversion num√©rique
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if not df[col].isnull().all():
                print(f"Conversion num√©rique r√©ussie pour: '{col}'")
                return col
        except:
            continue
    
    raise ValueError("Aucune colonne num√©rique valide trouv√©e pour la cible")

# 5. Nettoyage des donn√©es
def clean_data(df, target_col):
    """Nettoie et pr√©pare le DataFrame avec validation de la cible"""
    # Supprimer les colonnes vides
    df = df.dropna(axis=1, how='all')
    
    # Standardiser les noms de colonnes
    df.columns = [normalize_string(col) for col in df.columns]
    target_col = normalize_string(target_col)
    
    # V√©rifier la pr√©sence de la colonne cible
    if target_col not in df.columns:
        available_cols = "\n".join(df.columns)
        raise ValueError(f"Colonne cible '{target_col}' non trouv√©e apr√®s nettoyage. Colonnes disponibles:\n{available_cols}")
    
    # V√©rifier le type de la cible
    if not np.issubdtype(df[target_col].dtype, np.number):
        print(f"Conversion de la cible en num√©rique: '{target_col}'")
        try:
            # Tenter la conversion num√©rique
            df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
            
            # V√©rifier si la conversion a r√©ussi
            if df[target_col].isnull().all():
                raise ValueError(f"√âchec de conversion num√©rique pour '{target_col}'")
        except Exception as e:
            print(f"Erreur de conversion: {str(e)}")
            # Changer de cible si possible
            numeric_cols = df.select_dtypes(include=np.number).columns
            if target_col in numeric_cols:
                numeric_cols = numeric_cols.drop(target_col)
            if len(numeric_cols) > 0:
                new_target = numeric_cols[0]
                print(f"Changement de cible: '{target_col}' ‚Üí '{new_target}'")
                target_col = new_target
            else:
                raise ValueError("Aucune colonne num√©rique disponible pour la cible")
    
    # Identifier les colonnes num√©riques et cat√©gorielles
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=np.number).columns.tolist()
    
    # Exclure la colonne cible des caract√©ristiques
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    if target_col in categorical_cols:
        categorical_cols.remove(target_col)
    
    print("\nNettoyage des donn√©es:")
    print(f"Colonnes num√©riques: {len(numeric_cols)}")
    print(f"Colonnes cat√©gorielles: {len(categorical_cols)}")
    print(f"Colonne cible: '{target_col}'")
    
    # Gestion des valeurs manquantes
    imputer_num = SimpleImputer(strategy='median')
    imputer_cat = SimpleImputer(strategy='most_frequent')
    
    if numeric_cols:
        df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])
    if categorical_cols:
        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
    
    # Remplacer les valeurs manquantes dans la cible par la m√©diane
    if df[target_col].isnull().any():
        median_target = df[target_col].median()
        df[target_col].fillna(median_target, inplace=True)
        print(f"Valeurs manquantes dans la cible remplac√©es par la m√©diane: {median_target}")
    
    # Supprimer les colonnes avec peu de variance
    cols_to_drop = []
    for col in numeric_cols:
        if col in df.columns and df[col].nunique() <= 1:
            cols_to_drop.append(col)
    
    for col in categorical_cols:
        if col in df.columns and (df[col].nunique() == len(df) or df[col].nunique() == 1):
            cols_to_drop.append(col)
    
    if cols_to_drop:
        print(f"Suppression des colonnes √† faible variance: {cols_to_drop}")
        df = df.drop(columns=cols_to_drop)
    
    # Mettre √† jour les listes de colonnes
    numeric_cols = [col for col in numeric_cols if col in df.columns]
    categorical_cols = [col for col in categorical_cols if col in df.columns]
    
    # Supprimer les doublons
    initial_count = len(df)
    df = df.drop_duplicates()
    final_count = len(df)
    print(f"Doublons supprim√©s: {initial_count - final_count}")
    
    return df, target_col

# 6. Feature Engineering
def feature_engineering(df, target_col):
    """Cr√©e de nouvelles caract√©ristiques et s√©lectionne les meilleures"""
    # Cr√©er des caract√©ristiques temporelles
    time_cols = [col for col in df.columns if 'annee' in col or 'year' in col or 'date' in col]
    if time_cols:
        time_col = time_cols[0]
        df['time_norm'] = (df[time_col] - df[time_col].min()) / (df[time_col].max() - df[time_col].min())
    
    # Interactions entre caract√©ristiques
    area_cols = [col for col in df.columns if 'superficie' in col or 'area' in col or 'surface' in col]
    price_cols = [col for col in df.columns if 'prix' in col or 'price' in col or 'cost' in col]
    
    if area_cols and price_cols:
        area_col = area_cols[0]
        price_col = price_cols[0]
        df['area_price_ratio'] = df[area_col] * df[price_col]
    
    # S√©lection de caract√©ristiques bas√©e sur l'information mutuelle
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # Encoder les variables cat√©gorielles pour MI
    X_encoded = X.copy()
    for col in X_encoded.select_dtypes(include='object').columns:
        le = LabelEncoder()
        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))
    
    # Calculer les scores MI
    mi_scores = mutual_info_regression(X_encoded, y, random_state=config.SEED)
    mi_scores = pd.Series(mi_scores, index=X_encoded.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    
    # S√©lectionner les top caract√©ristiques
    top_n = min(20, len(mi_scores))
    top_features = mi_scores.head(top_n).index.tolist()
    df = df[top_features + [target_col]]
    
    # Sauvegarder l'importance des caract√©ristiques
    os.makedirs('results', exist_ok=True)
    mi_scores.to_csv('results/feature_importance.csv')
    
    print(f"\nTop {top_n} caract√©ristiques s√©lectionn√©es:")
    print(top_features)
    
    return df, target_col

# 7. Pr√©paration des donn√©es
def prepare_data(df, target_col):
    """Pr√©pare les donn√©es pour l'entra√Ænement"""
    # S√©parer les caract√©ristiques et la cible
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # S√©parer les types de caract√©ristiques
    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()
    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()
    
    # Encodage des variables cat√©gorielles
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le
        os.makedirs('models', exist_ok=True)
        joblib.dump(le, f'models/{col}_encoder.pkl')
    
    # Transformation num√©rique
    scaler = PowerTransformer()
    if numeric_cols:
        X[numeric_cols] = scaler.fit_transform(X[numeric_cols])
    joblib.dump(scaler, 'models/scaler.pkl')
    
    # Division des donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=config.TEST_SIZE, random_state=config.SEED
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=config.VAL_SIZE, random_state=config.SEED
    )
    
    print(f"\nFormes des donn√©es:")
    print(f"Entra√Ænement: {X_train.shape}")
    print(f"Validation: {X_val.shape}")
    print(f"Test: {X_test.shape}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# 8. Architecture du Mod√®le
def create_advanced_model(input_shape):
    """Cr√©e un mod√®le de deep learning avanc√©"""
    # Entr√©e pour les caract√©ristiques
    input_layer = Input(shape=(input_shape,))
    x = input_layer
    
    # Couches denses avec r√©gularisation
    x = Dense(256, activation='swish', 
              kernel_initializer=HeUniform(),
              kernel_regularizer=l1_l2(config.L1_REG, config.L2_REG))(x)
    x = BatchNormalization()(x)
    x = Dropout(config.DROPOUT_RATE)(x)
    
    x = Dense(192, activation='swish',
              kernel_initializer=HeUniform(),
              kernel_regularizer=l1_l2(config.L1_REG, config.L2_REG))(x)
    x = LayerNormalization()(x)
    x = Dropout(config.DROPOUT_RATE * 0.8)(x)
    
    x = Dense(128, activation='swish',
              kernel_initializer=HeUniform(),
              kernel_regularizer=l1_l2(config.L1_REG, config.L2_REG))(x)
    
    # M√©canisme d'attention
    attention = Dense(128, activation='softmax')(x)
    x = tf.keras.layers.multiply([x, attention])
    
    # Couche de sortie
    output = Dense(1, activation='linear')(x)
    
    model = Model(inputs=input_layer, outputs=output)
    
    # Optimiseur
    optimizer = Adam(
        learning_rate=config.LEARNING_RATE,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7
    )
    
    # CORRECTION: Utilisation explicite de la classe Huber
    model.compile(
        optimizer=optimizer,
        loss=Huber(),  # Utilisation directe de la classe de perte
        metrics=['mae', tf.keras.metrics.RootMeanSquaredError(name='rmse')]
    )
    
    return model

# 9. Entra√Ænement du mod√®le
def train_model(model, X_train, y_train, X_val, y_val):
    """Entra√Æne le mod√®le avec des callbacks avanc√©s"""
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=config.PATIENCE,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=7,
            min_lr=config.MIN_LR,
            verbose=1
        ),
        ModelCheckpoint(
            config.MODEL_SAVE_PATH,
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        )
    ]
    
    print("\nD√©but de l'entra√Ænement du mod√®le...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=config.MAX_EPOCHS,
        batch_size=config.BATCH_SIZE,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

# 10. √âvaluation et visualisation
def evaluate_model(model, X_test, y_test, history):
    """√âvalue le mod√®le et g√©n√®re des visualisations"""
    # Pr√©dictions
    print("\n√âvaluation sur l'ensemble de test...")
    y_pred = model.predict(X_test).flatten()
    
    # M√©triques
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mae = np.mean(np.abs(y_test - y_pred))
    
    print(f"\nPerformance finale:")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R¬≤: {r2:.4f}")
    
    # Courbe d'apprentissage
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Entra√Ænement')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.title('Courbe d\'apprentissage')
    plt.xlabel('√âpoques')
    plt.ylabel('Perte')
    plt.legend()
    plt.savefig('results/learning_curve.png')
    plt.close()
    print("Courbe d'apprentissage sauvegard√©e")
    
    # Pr√©dictions vs R√©alit√©
    plt.figure(figsize=(10, 8))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], 
             [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('Valeurs R√©elles')
    plt.ylabel('Pr√©dictions')
    plt.title('Pr√©dictions vs R√©alit√©')
    plt.grid(True)
    plt.savefig('results/predictions_vs_actual.png')
    plt.close()
    print("Graphique pr√©dictions vs r√©alit√© sauvegard√©")
    
    # Distribution des erreurs
    errors = y_test - y_pred
    plt.figure(figsize=(10, 6))
    sns.histplot(errors, kde=True)
    plt.title('Distribution des Erreurs de Pr√©diction')
    plt.xlabel('Erreur')
    plt.savefig('results/error_distribution.png')
    plt.close()
    print("Distribution des erreurs sauvegard√©e")
    
    # Sauvegarder les m√©triques
    metrics = {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'best_epoch': len(history.history['loss']) - config.PATIENCE
    }
    
    with open('results/model_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=4)
    
    return metrics

# 11. Sauvegarde compl√®te du mod√®le
def save_full_model(model, target_col, X_train):
    """Sauvegarde le mod√®le et tous ses artefacts dans un format pr√™t pour la production"""
    # 1. Sauvegarde au format HDF5 (pour Keras)
    model.save(config.MODEL_SAVE_PATH)
    print(f"Mod√®le sauvegard√© (HDF5): {config.MODEL_SAVE_PATH}")
    
    # 2. Sauvegarde au format SavedModel (pour TF Serving)
    saved_model_path = "models/saved_model"
    tf.saved_model.save(model, saved_model_path)
    print(f"Mod√®le sauvegard√© (SavedModel): {saved_model_path}")
    
    # 3. Sauvegarde des m√©tadonn√©es
    feature_names = X_train.columns.tolist()
    
    metadata = {
        'target_column': target_col,
        'feature_names': feature_names,
        'model_type': 'regression',
        'input_shape': model.input_shape[1:],
        'output_shape': model.output_shape[1:]
    }
    
    with open('models/model_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=4)
    
    print("M√©tadonn√©es du mod√®le sauvegard√©es")
    
    # 4. Cr√©ation d'un package zip pour le d√©ploiement
    shutil.make_archive('production_model', 'zip', 'models')
    print("Package de d√©ploiement cr√©√©: production_model.zip")
    
    return feature_names

# 12. Pipeline principal
def main():
    print("="*80)
    print("üöÄ D√âMARRAGE DU PIPELINE DE DEEP LEARNING AVANC√â")
    print("="*80)
    
    # Cr√©er les r√©pertoires n√©cessaires
    os.makedirs("models", exist_ok=True)
    os.makedirs("results", exist_ok=True)
    os.makedirs("data", exist_ok=True)
    
    try:
        # √âtape 1: Inspection des fichiers
        print("\n" + "="*80)
        print("√âTAPE 1: INSPECTION DES FICHIERS CSV")
        print("="*80)
        column_report = inspect_csv_files()
        
        # √âtape 2: Chargement des donn√©es
        print("\n" + "="*80)
        print("√âTAPE 2: CHARGEMENT DES DONN√âES")
        print("="*80)
        df = load_all_data()
        
        # √âtape 3: S√©lection de la cible
        print("\n" + "="*80)
        print("√âTAPE 3: S√âLECTION DE LA COLONNE CIBLE")
        print("="*80)
        target_col = select_target_column(df)
        print(f"Colonne cible s√©lectionn√©e: '{target_col}'")
        
        # √âtape 4: Nettoyage des donn√©es
        print("\n" + "="*80)
        print("√âTAPE 4: NETTOYAGE DES DONN√âES")
        print("="*80)
        df_clean, target_col = clean_data(df, target_col)
        print("\nAper√ßu des donn√©es nettoy√©es:")
        print(df_clean.head())
        
        # √âtape 5: Feature Engineering
        print("\n" + "="*80)
        print("√âTAPE 5: FEATURE ENGINEERING")
        print("="*80)
        df_fe, target_col = feature_engineering(df_clean, target_col)
        print("\nAper√ßu des donn√©es transform√©es:")
        print(df_fe.head())
        
        # √âtape 6: Pr√©paration des donn√©es
        print("\n" + "="*80)
        print("√âTAPE 6: PR√âPARATION DES DONN√âES")
        print("="*80)
        X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(df_fe, target_col)
        
        # √âtape 7: Construction du mod√®le
        print("\n" + "="*80)
        print("√âTAPE 7: CONSTRUCTION DU MOD√àLE")
        print("="*80)
        model = create_advanced_model(X_train.shape[1])
        model.summary()
        
        # √âtape 8: Entra√Ænement
        print("\n" + "="*80)
        print("√âTAPE 8: ENTRA√éNEMENT DU MOD√àLE")
        print("="*80)
        history = train_model(model, X_train, y_train, X_val, y_val)
        
        # √âtape 9: √âvaluation
        print("\n" + "="*80)
        print("√âTAPE 9: √âVALUATION DU MOD√àLE")
        print("="*80)
        metrics = evaluate_model(model, X_test, y_test, history)
        
        # √âtape 10: Sauvegarde finale
        print("\n" + "="*80)
        print("√âTAPE 10: SAUVEGARDE FINALE")
        print("="*80)
        feature_names = save_full_model(model, target_col, X_train)
        
        print("\n" + "="*80)
        print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!")
        print("="*80)
        print(f"Le mod√®le est pr√™t √† √™tre utilis√© dans le dossier: {os.path.abspath('models')}")
        print(f"Caract√©ristiques utilis√©es: {feature_names}")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {str(e)}")
        print("="*80)
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()